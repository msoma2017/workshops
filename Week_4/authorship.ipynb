{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Feature Extraction\n",
    "\n",
    "### What Features are appropriate?\n",
    "\n",
    "Perhaps the list of unique words?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'first',\n",
       " 'is',\n",
       " 'one.',\n",
       " 'second',\n",
       " 'the',\n",
       " 'third',\n",
       " 'this',\n",
       " 'tweet.',\n",
       " 'tweet?']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = [  'This is the first tweet.', 'This is the second second tweet.','And the third one.','Is this the first tweet?'] \n",
    "vocab = [x.lower() for tweet in tweets for x in tweet.split() ]\n",
    "vocab = list(set(vocab))\n",
    "sorted(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "\n",
    "vectorization the general process of turning a collection of text documents into numerical feature vectors.\n",
    "\n",
    "### CountVectorizer\n",
    "\n",
    "Count Vectorizer is useful for tokenizing. In Sklearn tokenizing strings gives an integer id for each possible token, for instance by using white-spaces and punctuation as token separators. CountVectorizer counts the number of token occurrences i.e. the number of times a token appears\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'first', 'is', 'one', 'second', 'the', 'third', 'this', 'tweet']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(tweets)\n",
    "features = vectorizer.get_feature_names()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 0, 1, 0, 1, 1],\n",
       "       [0, 0, 1, 0, 2, 1, 0, 1, 1],\n",
       "       [1, 0, 0, 1, 0, 1, 1, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 1, 0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  transform \n",
    ".transform changes the text to a list ot features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0, 0, 1, 1]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(['Lets assume this is a new tweet']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-Grams and Tri-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'and the', 'first', 'first tweet', 'is', 'is the', 'is this', 'one', 'second', 'second second', 'second tweet', 'the', 'the first', 'the second', 'the third', 'third', 'third one', 'this', 'this is', 'this the', 'tweet']\n"
     ]
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), min_df=1)\n",
    "\n",
    "X = bigram_vectorizer.fit_transform(tweets)\n",
    "features_bi = bigram_vectorizer.get_feature_names()\n",
    "print(sorted(features_bi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'and the', 'and the third', 'first', 'first tweet', 'is', 'is the', 'is the first', 'is the second', 'is this', 'is this the', 'one', 'second', 'second second', 'second second tweet', 'second tweet', 'the', 'the first', 'the first tweet', 'the second', 'the second second', 'the third', 'the third one', 'third', 'third one', 'this', 'this is', 'this is the', 'this the', 'this the first', 'tweet']\n"
     ]
    }
   ],
   "source": [
    "trigram_vectorizer = CountVectorizer(ngram_range=(1, 3),  min_df=1)\n",
    "\n",
    "X = trigram_vectorizer.fit_transform(tweets)\n",
    "features_tri = trigram_vectorizer.get_feature_names()\n",
    "print(sorted(features_tri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problems:\n",
    "1. Longer documents will have higher average count values than shorter documents\n",
    "2. Some words are very common e.g. 'the', 'and', 'is' will automatically have higher counts\n",
    "\n",
    "#### Solution:\n",
    "1. Term Frequencies times Inverse Document Frequency\n",
    "\n",
    "### TF-idf -  \n",
    "Term Frequencies almost look at words as a percentage of the total i.e. Instead of -the word 'kenya' was used 100 times, TF says the word 'kenya' was used 2% of the time.\n",
    "\n",
    "To take care of the second problem, we use IDF.\n",
    "Inverse Document Frequency factor is a way of diminishing the weight of terms that occur very frequently in a document set and increasing the weight of terms that occur rarely\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 31)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Importations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a.) Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [], 'target': [], 'target_names': ['UKenyatta', 'RailaOdinga']}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a dictionary for storing the data\n",
    "my_data = {'data':[],\n",
    "          'target':[],\n",
    "          'target_names':[]}\n",
    "\n",
    "mytarget = ['UKenyatta','RailaOdinga']\n",
    "\n",
    "my_data['target_names'] = mytarget\n",
    "my_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37877</th>\n",
       "      <td>b'The roads constructed across the country, pr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25822</th>\n",
       "      <td>b'We were hosted by Mzee Ole Kinayi who was ge...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6526</th>\n",
       "      <td>b'Attending Sunday service earlier at ACK Cath...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38426</th>\n",
       "      <td>b'Doctors to be paid for days they were on str...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38825</th>\n",
       "      <td>b'RT @WilliamsRuto: We must hold together, pre...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet  class\n",
       "37877  b'The roads constructed across the country, pr...      0\n",
       "25822  b'We were hosted by Mzee Ole Kinayi who was ge...      1\n",
       "6526   b'Attending Sunday service earlier at ACK Cath...      1\n",
       "38426  b'Doctors to be paid for days they were on str...      0\n",
       "38825  b'RT @WilliamsRuto: We must hold together, pre...      0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the data\n",
    "nrs = pd.read_csv('kenya.csv')\n",
    "nrs = shuffle(shuffle(nrs))\n",
    "nrs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_data['data'] = list(nrs['tweet'])\n",
    "my_data['target'] = list(nrs['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Split the data in training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = my_data['data'][:-1000]\n",
    "y_train = my_data['target'][:-1000]\n",
    "\n",
    "X_test = my_data['data'][-1000:]\n",
    "y_test = my_data['target'][-1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.) Create and Train a classifier\n",
    "#### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47840, 14780)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Occurences\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47840, 14780)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Frequencies\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training a classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of our model is:\n",
      "0.984\n",
      "Confusion matrix:\n",
      "[[306  15]\n",
      " [  1 678]]\n"
     ]
    }
   ],
   "source": [
    "X_tests_counts = count_vect.transform(X_test)\n",
    "X_tests_tfidf = tfidf_transformer.transform(X_tests_counts)\n",
    "expected  = y_test\n",
    "predicted = clf.predict(X_tests_tfidf)\n",
    "print(\"Accuracy of our model is:\\n%s\" % metrics.accuracy_score(expected, predicted))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'Waived debts for rice farmers in the Mwea Irrigation Scheme as we continue to extend our support to farmers in every part of this country' ===> UKenyatta\n",
      "\n",
      "\"Kenyans don't want handouts. They need a hand up initiative from their government. The economy must grow for all - not the few.\" ===> RailaOdinga\n",
      "\n",
      "'today is a beautiful day' ===> RailaOdinga\n"
     ]
    }
   ],
   "source": [
    "#Predicting Outcome\n",
    "tweet1 = 'Waived debts for rice farmers in the Mwea Irrigation Scheme as we continue to extend our support to farmers in every part of this country' #\n",
    "tweet2 = 'Kenyans don\\'t want handouts. They need a hand up initiative from their government. The economy must grow for all - not the few.'\n",
    "tweet3 = 'today is a beautiful day'\n",
    "\n",
    "tweets_new = [tweet1,tweet2,tweet3]\n",
    "X_new_counts = count_vect.transform(tweets_new)\n",
    "\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for tw, category in zip(tweets_new, predicted):\n",
    "    print('\\n%r ===> %s' % (tw, my_data['target_names'][category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
